{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# From eLabFTW to NOMAD\n",
        "\n",
        "In this notebook I will show how to transfer metadata from eLabFTW to NOMAD, as described in the metadata pipeline presented in the thesis. I will use both the eLabFTW and NOMAD API.\n",
        "\n",
        "<u>Prerequisite</u>: eLabFTW experiment with the metadata\n",
        "\n",
        "<u>Steps of the procedure</u>:\n",
        "1. Write a YAML Schema Package (The one we used can be find below).\n",
        "2. GET the experiment from eLabFTW.\n",
        "3. Create the Python dictionaries (that will be turned into JSON files) and edit them with the relevant metadata.\n",
        "4. Prepare the JSON files for the upload in one zip folder.\n",
        "5. Upload to NOMAD via API.\n",
        "\n",
        "<u>Disclaimer</u>\n",
        "\n",
        "In this notebook, for simplicity, I will only consider some of the metadata values coming from the pipeline. Yet, I deem them to be representative of the kind of work that should be done for transfering metadata from eLabFTW to NOMAD.\n",
        "\n",
        "In particular, we will consider the metadata coming from the Python program. That is, the value of the following eLabFTW keys:\n",
        "- `Experiment_DateAndTime`\n",
        "- `Experiment_RunNumber`\n",
        "- `Experiment_MeasurementParameters_field_direction`\n",
        "- `UserCase_Sample_sample_name`\n",
        "- `Experiment_MeasurementParameters_current_pulse` (mA)\n",
        "- `Experiment_MeasurementParameters_magnetic_field_sequence` (V)\n",
        "\n",
        "Moreover, since we are intersted in describing the sample, we will consider some of the metadata entered by the user in eLabFTW, namely those values that describe the layers of the thin film (i.e., the sample) and the device. That is, the value of the following eLabFTW keys:\n",
        "- `UserCase_Sample_layer_1_thickness` (nm)\n",
        "- `UserCase_Sample_layer_2_thickness` (nm)\n",
        "- `UserCase_Sample_layer_3_thickness` (nm)\n",
        "- `UserCase_Sample_layer_4_thickness` (nm)\n",
        "- `UserCase_Sample_layer_5_thickness` (nm)\n",
        "- `UserCase_SampleMaterialProperties_layer_1_material`\n",
        "- `UserCase_SampleMaterialProperties_layer_2_material`\n",
        "- `UserCase_SampleMaterialProperties_layer_3_material`\n",
        "- `UserCase_SampleMaterialProperties_layer_4_material`\n",
        "- `UserCase_SampleMaterialProperties_layer_5_material`\n",
        "- `UserCase_Sample_device_geometry`\n",
        "- `UserCase_Sample_device_width`(mm)\n",
        "- `UserCase_Sample_device_length` (mm)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "AMGI9n6UknCT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Writing a YAML Schema Package for NOMAD\n",
        "\n",
        "In this part we should write the YAML Schema package for NOMAD. (Here the documentation on how to build this kind of schema: https://nomad-lab.eu/prod/v1/docs/howto/customization/basics.html). Here is a YT tutorial by FAIRmat: https://www.youtube.com/watch?v=5VXGZNlz9rc).\n",
        "\n",
        "The YAML schema package used in this code is available in this repository:https://github.com/filippo-vasone/FAIR-metadata-pipeline-for-experimental-matsci/blob/main/SOT.schema.archive.yaml.\n",
        "\n",
        "(See the thesis for comment on the YAML Schema Package).\n",
        "\n",
        "There are three section definitions:\n",
        "- `SampleStructure`\n",
        "- `SOTMeasurement`\n",
        "- `Substance`\n",
        "\n",
        "```\n",
        "definitions:\n",
        "    name: SOT\n",
        "    sections:\n",
        "        SampleStructure:  \n",
        "          base_sections:\n",
        "            - nomad.datamodel.metainfo.basesections.CompositeSystem\n",
        "            - nomad.datamodel.data.EntryData\n",
        "          quantities:\n",
        "            UserCase_Sample_device_width:\n",
        "              type: np.float64\n",
        "              unit: mm\n",
        "              m_annotations:\n",
        "                eln:\n",
        "                  component: NumberEditQuantity\n",
        "                  display:\n",
        "                    unit: mm\n",
        "            UserCase_Sample_device_length:\n",
        "              type: np.float64\n",
        "              unit: mm\n",
        "              m_annotations:\n",
        "                eln:\n",
        "                  component: NumberEditQuantity\n",
        "                  display:\n",
        "                    unit: mm\n",
        "            UserCase_Sample_device_geometry:\n",
        "              type: str\n",
        "              m_annotations:\n",
        "                eln:\n",
        "                  component: StringEditQuantity\n",
        "          sub_sections:\n",
        "            layers:\n",
        "              section:\n",
        "                quantities:\n",
        "                  layer:\n",
        "                    type: Substance\n",
        "                    m_annotations:\n",
        "                      eln:\n",
        "                        component: ReferenceEditQuantity\n",
        "              repeats: true\n",
        "\n",
        "        SOTMeasurement:\n",
        "            base_sections:\n",
        "               - nomad.datamodel.metainfo.eln.ElnBaseSection\n",
        "               - nomad.datamodel.data.EntryData\n",
        "               - nomad.datamodel.metainfo.basesections.CompositeSystemReference\n",
        "            quantities:\n",
        "                UserCase_Sample_sample_name:\n",
        "                  type: str\n",
        "                  m_annotations:\n",
        "                      eln:\n",
        "                        component: StringEditQuantity\n",
        "                Experiment_RunNumber:\n",
        "                  type: int\n",
        "                  m_annotations:\n",
        "                      eln:\n",
        "                        component: NumberEditQuantity\n",
        "                Experiment_MeasurementParameters_field_direction:\n",
        "                  type: str\n",
        "                  m_annotations:\n",
        "                      eln:\n",
        "                        component: StringEditQuantity\n",
        "                Experiment_MeasurementParameters_current_pulse:\n",
        "                  type: np.float64\n",
        "                  unit: mA\n",
        "                  m_annotations:\n",
        "                    eln:\n",
        "                      component: NumberEditQuantity\n",
        "                      display:\n",
        "                        unit: mA                \n",
        "                Experiment_MeasurementParameters_magnetic_field_sequence:\n",
        "                  type: str # this is a string for simplicity\n",
        "                  unit: V\n",
        "                  m_annotations:\n",
        "                      eln:\n",
        "                        component: StringEditQuantity\n",
        "                        display:\n",
        "                          unit: V                \n",
        "\n",
        "        Substance:\n",
        "          base_sections:\n",
        "          - nomad.datamodel.metainfo.eln.Substance\n",
        "          - nomad.datamodel.data.EntryData\n",
        "          quantities:\n",
        "            UserCase_Sample_layer_thickness:\n",
        "              type: np.float64\n",
        "              unit: nm\n",
        "              m_annotations:\n",
        "                eln:\n",
        "                  component: NumberEditQuantity\n",
        "                  display:\n",
        "                    unit: nm\n",
        "```"
      ],
      "metadata": {
        "id": "Y3xarPIk5gRZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This schema has to be saved in a YAML file to be uploaded into the same folder of this Notebook with the name `SOT.schema.archive.yaml`"
      ],
      "metadata": {
        "id": "MTdZywYjmQvI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. GET the experiment metadata from eLabFTW\n",
        "\n",
        "Now we use the eLabFTW API to retrieve the metadata from the experiment with id = 463."
      ],
      "metadata": {
        "id": "9YAW2yRFBjbl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# install the elab library\n",
        "!pip install elabapi-python\n",
        "\n",
        "# import modules\n",
        "import time\n",
        "import json\n",
        "import elabapi_python\n",
        "\n",
        "# replace with your api key (obtained from User Panel > API Keys)\n",
        "my_api_key = '***' # censored\n",
        "\n",
        "# START CONFIGURATION\n",
        "\n",
        "configuration = elabapi_python.Configuration()\n",
        "configuration.api_key['api_key'] = my_api_key\n",
        "configuration.api_key_prefix['api_key'] = 'Authorization'\n",
        "configuration.host = 'https://***//api/v2' # censored\n",
        "configuration.debug = False\n",
        "configuration.verify_ssl = False\n",
        "\n",
        "# create an instance of the API class\n",
        "api_client = elabapi_python.ApiClient(configuration)\n",
        "# fix issue with Authorization header not being properly set by the generated lib\n",
        "api_client.set_default_header(header_name='Authorization', header_value=my_api_key)\n",
        "\n",
        "# END CONFIGURATION"
      ],
      "metadata": {
        "id": "T_xXSYfc-h-D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# get the EXPERIMENT instance\n",
        "from pprint import pprint\n",
        "api_exp_instance = elabapi_python.ExperimentsApi(api_client)\n",
        "id = 463\n",
        "\n",
        "# Read the experiment\n",
        "api_response = api_exp_instance.get_experiment(id)\n",
        "pprint(api_response)"
      ],
      "metadata": {
        "id": "iXP4E8gIDcYK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# get the metadata of the experiment as a Python dictionary\n",
        "md_exp_dic = json.loads(api_response.metadata)\n",
        "\n",
        "print(md_exp_dic)"
      ],
      "metadata": {
        "id": "Gumw6PLBSbxU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Create the Python dictionaries\n",
        "\n",
        "In this step we will create several Python dictionaries, which will then be turned into JSON files to be uploaded as entries in NOMAD.\n",
        "\n",
        "We will hard code the dictionaries with the structure of a NOMAD entry and edit them with the relevant metadata from eLabFTW.\n",
        "\n"
      ],
      "metadata": {
        "id": "1yCPT3dfvw1k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.1 We create the dictionaries for the layers of the thin film\n",
        "These dictionaries will be transformed into JSON files to be uploaded as raw files on NOMAD. For the files to be processed correctly, they need to have the right structure.\n",
        "\n",
        "This structure is hard coded and it is inferred from other downloaded NOMAD raw files.\n",
        "\n",
        "The structure specifies that the raw JSON file should follow the 'Substance' definition from the NOMAD Schema package.\n",
        "\n",
        "Then, we will update the dictionary with the metadata coming from the experiment."
      ],
      "metadata": {
        "id": "iLDhVyZt-X9L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# empty dictionary in which we will store each layer dictionary\n",
        "layers ={}\n",
        "\n",
        "for n in range(1,6):\n",
        "  # we hard code a base dictionary (taken from the structure of a NOMAD raw entry)\n",
        "  base = {\"data\":{\"m_def\":\"../upload/raw/SOT.schema.archive.yaml#/definitions/section_definitions/2\",\"name\":\"\"}}\n",
        "\n",
        "  material = md_exp_dic['extra_fields'][f'UserCase_SampleMaterialProperties_layer_{n}_material']['value'] #the name of the material of the layer from the metadata\n",
        "  thickness = md_exp_dic['extra_fields'][f'UserCase_Sample_layer_{n}_thickness']['value'] #the thickness of the material from the metadata\n",
        "  thickness = float(thickness) # turn the thickness into a float for metadata quality reasons\n",
        "\n",
        "  # we update the base dictionary with the values from the metadata\n",
        "  base['data']['name'] = material\n",
        "  base['data']['UserCase_Sample_layer_thickness'] = thickness\n",
        "  # we add the updated layer dictionary to the dictionary of layers\n",
        "  layers[f'{material}']= base\n",
        "\n",
        "print(layers)\n",
        "\n",
        "# data quality check to check if the key of each nested dictionary corresponds to the value of 'name' inside it\n",
        "for key, value in layers.items():\n",
        "  if key == value['data']['name']:\n",
        "    print(\"All good\")\n",
        "  else:\n",
        "    print(\"Something is wrong\")"
      ],
      "metadata": {
        "id": "N1YA79JsnXtB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.2 We create the dictionary for the thin film (SampleStructure)\n"
      ],
      "metadata": {
        "id": "i1gF68W1GqRv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# As above, we first hard code the structured dictionary for SampleStructure\n",
        "\n",
        "thin_film = {\"data\":{\"m_def\":\"../upload/raw/SOT.schema.archive.yaml#/definitions/section_definitions/0\",\"name\":\"thin_film\"}}\n",
        "\n",
        "# then we update this dictionary with the value from the eLabFTW metadata\n",
        "\n",
        "thin_film['data']['UserCase_Sample_device_width'] = float(md_exp_dic['extra_fields']['UserCase_Sample_device_width']['value']) # for metadata quality, I turn this value into a float\n",
        "thin_film['data']['UserCase_Sample_device_length'] = float(md_exp_dic['extra_fields']['UserCase_Sample_device_length']['value']) # for metadata quality, I turn this value into a float\n",
        "thin_film['data']['UserCase_Sample_device_geometry'] = md_exp_dic['extra_fields']['UserCase_Sample_device_geometry']['value']\n",
        "\n",
        "print(thin_film)"
      ],
      "metadata": {
        "id": "t9fYVdB2Kjd0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.3 We create the dictionary for the experiment (SOTMeasurement)"
      ],
      "metadata": {
        "id": "lOzSyoTMNL48"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# As above, we first hard code the structured dictionary for SOTMeasurement\n",
        "\n",
        "exp_entry = {\"data\":{\"m_def\":\"../upload/raw/SOT.schema.archive.yaml#/definitions/section_definitions/1\"}}\n",
        "\n",
        "\n",
        "# then we update this dictionary with the values from the eLabFTW metadata\n",
        "\n",
        "# the name of the experiment is the 'title' in eLabFTW and thus it comes directly from the API response and not from the metadata dictionary\n",
        "exp_entry['data']['name']= api_response.title\n",
        "\n",
        "# I establish a list of the keys whose value I want to transfer between dictionaries\n",
        "key_list_exp = ['UserCase_Sample_sample_name',\n",
        "                'Experiment_RunNumber',\n",
        "                'Experiment_MeasurementParameters_field_direction',\n",
        "                'Experiment_MeasurementParameters_current_pulse',\n",
        "                'Experiment_MeasurementParameters_magnetic_field_sequence']\n",
        "\n",
        "# I update the exp_entry dict with the values coming from the metadata dictionary\n",
        "for key in key_list_exp:\n",
        "  exp_entry[\"data\"][key] = md_exp_dic[\"extra_fields\"][key][\"value\"]\n",
        "\n",
        "# Since NOMAD (unlike eLabFTW) does not support units for strings, add units as part of string for Experiment_MeasurementParameters_magnetic_field_sequence (V)\n",
        "exp_entry[\"data\"]['Experiment_MeasurementParameters_magnetic_field_sequence'] = f'{md_exp_dic[\"extra_fields\"][\"Experiment_MeasurementParameters_magnetic_field_sequence\"][\"value\"]}{md_exp_dic[\"extra_fields\"][\"Experiment_MeasurementParameters_magnetic_field_sequence\"][\"unit\"]}'\n",
        "\n",
        "# for metadata quality, turn the value of Experiment_RunNumber into integer\n",
        "exp_entry[\"data\"][\"Experiment_RunNumber\"] = int(exp_entry[\"data\"][\"Experiment_RunNumber\"])\n",
        "\n",
        "# for metadata quality, turn the value of Experiment_MeasurementParameters_current_pulse into float\n",
        "exp_entry[\"data\"][\"Experiment_MeasurementParameters_current_pulse\"] = float(exp_entry[\"data\"][\"Experiment_MeasurementParameters_current_pulse\"])\n",
        "\n",
        "print(exp_entry)"
      ],
      "metadata": {
        "id": "uXiMDINgNRpG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Metadata quality: add datetime to `exp_entry` with the right format for NOMAD (ISO 8601)\n",
        "Example of NOMAD datetime (inferred): '2025-02-11T22:31:54.856891+00:00'\n",
        "\n",
        "We want the datetime we insert in `exp_entry` from eLabFTW to have the same structure as the above (ISO 8601 structure at UTC)"
      ],
      "metadata": {
        "id": "_IfoIKE12iPg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import necessary modules\n",
        "from datetime import datetime\n",
        "\n",
        "# we have a look at the datetime from eLabFTW\n",
        "time_elab = md_exp_dic[\"extra_fields\"][\"Experiment_DateAndTime\"][\"value\"]\n",
        "print(f'The original datetime from eLabFTW is: {time_elab} - with type {type(time_elab)}')\n",
        "\n",
        "# we transform the string into a Python datetime object\n",
        "elab_date_obj = datetime.strptime(time_elab, '%Y-%m-%d %H:%M:%S.%f')\n",
        "print(f'Now it is Python datetime object: {elab_date_obj} - with type {type(elab_date_obj)}')"
      ],
      "metadata": {
        "id": "xhxFeaIvYWPj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import necessary modules\n",
        "from datetime import tzinfo, timezone, date\n",
        "from zoneinfo import ZoneInfo\n",
        "\n",
        "# we check that the Python datetime object is a naive datetime object (i.e., without info on the time zone)\n",
        "print(f'The timezone of our datetime is: {elab_date_obj.tzinfo}')\n",
        "\n",
        "# we transform the datetime object from naive to aware, by adding the current time zone (but without changing the datetime)\n",
        "elab_date_obj_tz = elab_date_obj.replace(tzinfo=ZoneInfo(\"Europe/Rome\"))\n",
        "print(f'Now we added the timezone information. Thus the date is now: {elab_date_obj_tz} with this timezone: {elab_date_obj_tz.tzinfo}')\n",
        "\n",
        "# we convert the datetime from the current time zone to UTC\n",
        "elab_date_obj_utc = elab_date_obj_tz.astimezone(timezone.utc)\n",
        "print(f'In UTC, our datetime is the following: {elab_date_obj_utc}')\n",
        "\n",
        "# convert it to isoformat\n",
        "elab_iso_date_utc = elab_date_obj_utc.isoformat()\n",
        "print(f'In ISO 8601 format, the datetime is: {elab_iso_date_utc}')\n",
        "\n",
        "# we update the exp_entry dictionary\n",
        "exp_entry[\"data\"][\"datetime\"] = elab_iso_date_utc\n",
        "print(f'Here is our updated dictionary: {exp_entry}')"
      ],
      "metadata": {
        "id": "pzmzOT496_mA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Prepare the JSON files for the upload in one zip folder.\n"
      ],
      "metadata": {
        "id": "NkywpmkQUs_6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we have to do the following steps:\n",
        "- Move the YAML Schema into the same folder of this notebook. (In Google Colab: load it into the environnment).\n",
        "- Create an empty folder in the same folder of this notebook.\n",
        "- Move the YAML Schema in the folder\n",
        "- Transform each of the Python dictionaries that I created in the last section into JSON files. Disclaimer: we have to 'unpack' the dictionary `layers` because we would like to create a JSON file for each nested dictionary inside `layers`.\n",
        "- Move all these JSON files into the folder.\n",
        "- ZIP the folder."
      ],
      "metadata": {
        "id": "qfYBB-4a_L0S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import necessary modules\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "# create an empty folder\n",
        "folder_name = \"SOT_exp_API\"\n",
        "os.makedirs(folder_name, exist_ok=True)\n",
        "\n",
        "# upload of the YAML schema into Google Colab/Move the YAML Schema into the same folder of this notebook\n",
        "\n",
        "# move the YAML Schema into the folder\n",
        "schema = \"SOT.schema.archive.yaml\"\n",
        "shutil.move(schema, os.path.join(folder_name, schema))"
      ],
      "metadata": {
        "id": "My2P8I62PAw9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# transform the dictionaries created in sec. 3 into JSON files and move them to the folder\n",
        "\n",
        "# I tranform the dict 'exp_entry' into a file and move it in the folder\n",
        "\n",
        "# filename for the JSON file ('archive.json' as needed for NOMAD)\n",
        "file_name_exp_entry = exp_entry[\"data\"][\"name\"]+\".archive.json\"\n",
        "\n",
        "# tranform it into a JSON file\n",
        "with open(file_name_exp_entry, \"w\") as filesotexp:\n",
        "    json.dump(exp_entry, filesotexp)\n",
        "\n",
        "# move the file to the folder\n",
        "shutil.move(file_name_exp_entry, os.path.join(folder_name, file_name_exp_entry))"
      ],
      "metadata": {
        "id": "C8oJtbR_QP4k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# I tranform the dict 'thin_film' into a file and move it in the folder\n",
        "\n",
        "# filename for the JSON file ('archive.json' as needed for NOMAD)\n",
        "file_name_thin_film = thin_film[\"data\"][\"name\"]+\".archive.json\"\n",
        "\n",
        "# tranform it into a JSON file\n",
        "with open(file_name_thin_film, \"w\") as filesotexp:\n",
        "    json.dump(thin_film, filesotexp)\n",
        "\n",
        "# move the file to the folder\n",
        "shutil.move(file_name_thin_film, os.path.join(folder_name, file_name_thin_film))"
      ],
      "metadata": {
        "id": "UEC_kKSQRGU-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create a JSON file for each nested dictionary inside `layers`.\n",
        "\n",
        "for layer, content in layers.items():\n",
        "    filename = f'{layer}'+'.archive.json' # filename for the JSON files\n",
        "    with open(filename, \"w\") as filesotexp:\n",
        "        json.dump(content, filesotexp)       # create a JSON file for each dictionary\n",
        "    shutil.move(filename, os.path.join(folder_name, filename)) # move the JSON file to the folder"
      ],
      "metadata": {
        "id": "sgP4OQR2Sf3P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# zip the folder\n",
        "output_zip = \"SOT_exp_API.zip\"\n",
        "shutil.make_archive(output_zip.replace(\".zip\", \"\"), 'zip', folder_name)"
      ],
      "metadata": {
        "id": "92qLWvBCXNah"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Upload the ZIP folder to NOMAD via API\n",
        "\n",
        "The functions `get_authentication_token` and `upload_to_NOMAD` used in this Notebook come from here: https://github.com/Master-Data-Management-and-Curation/Tools-Data-Management-Curation/blob/main/nomad/NOMAD_API.py"
      ],
      "metadata": {
        "id": "SRe6XUizYFPz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import necessary modules\n",
        "import requests\n",
        "import pprint\n",
        "\n",
        "nomad_url = 'https://nomad-lab.eu/prod/v1/oasis/api/v1/' #public NOMAD oasis\n",
        "\n",
        "# nomad authentication credentials\n",
        "username='***' # censored\n",
        "password='***' # censored\n",
        "\n",
        "# function to get the authentication token\n",
        "def get_authentication_token(nomad_url, username, password):\n",
        "    '''Get the token for accessing your NOMAD unpublished uploads remotely'''\n",
        "    try:\n",
        "        response = requests.get(\n",
        "            nomad_url + 'auth/token', params=dict(username=username, password=password), timeout=10)\n",
        "        token = response.json().get('access_token')\n",
        "        if token:\n",
        "            return token\n",
        "\n",
        "        print('response is missing token: ')\n",
        "        print(response.json())\n",
        "        return\n",
        "    except Exception:\n",
        "        print('something went wrong trying to get authentication token')\n",
        "        return\n",
        "\n",
        "# get the authentication token\n",
        "token = get_authentication_token(nomad_url, username, password)"
      ],
      "metadata": {
        "id": "aq8wNRQiWoU4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# function to upload to NOMAD using API\n",
        "def upload_to_NOMAD(nomad_url, token, upload_file, upload_name, embargo_length=0):\n",
        "    '''Upload a single file for NOMAD upload, e.g., zip format'''\n",
        "    with open(upload_file, 'rb') as f:\n",
        "        try:\n",
        "            response = requests.post(\n",
        "                nomad_url + 'uploads?upload_name=' + upload_name + '&embargo_length=' + str(embargo_length),\n",
        "                headers={'Authorization': f'Bearer {token}', 'Accept': 'application/json'},\n",
        "                data=f, timeout=30)\n",
        "            upload_id = response.json().get('upload_id')\n",
        "            if upload_id:\n",
        "                return upload_id\n",
        "\n",
        "            print('response is missing upload_id: ')\n",
        "            print(response.json())\n",
        "            return\n",
        "        except Exception:\n",
        "            print('something went wrong uploading to NOMAD')\n",
        "            return"
      ],
      "metadata": {
        "id": "M2xKAfg1ZPeM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# upload the created folder to NOMAD\n",
        "upload_to_NOMAD(nomad_url, token, \"SOT_exp_API.zip\", \"upload_zip_sot_def\", embargo_length=0)"
      ],
      "metadata": {
        "id": "2mktG4ogaNHA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
